{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import current_timestamp\n\n\ndef add_ingestion_date(input_df):\n    output_df = input_df.withColumn(\"ingestion_date\", current_timestamp())\n    return output_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3c75d422-92c7-4bf2-b04d-11823e2afabe","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def re_arrange_partition_column(input_df, partition_column):\n    \n    sorted_columns = []\n    column_names = input_df.columns\n    \n    for column in column_names:\n        if column != partition_column:\n            sorted_columns.append(column)\n    \n    sorted_columns.append(partition_column)\n    output_df = input_df.select(sorted_columns)\n    \n    return output_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6bce4cb5-4704-440b-98a3-b2e6a0014612","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def overwrite_partition(input_df, db_name, table_name, partition_column):\n    output_df = re_arrange_partition_column(input_df, partition_column)\n    \n    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n    if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n        output_df.write.mode(\"overwrite\").insertInto(f\"{db_name}.{table_name}\")\n    else:\n        output_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"parquet\").saveAsTable(f\"{db_name}.{table_name}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"839337ca-c9e4-4788-9a5a-a2958afc48f0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def merge_delta_data(input_df, db_name, table_name, folder_path, merge_condition, partition_column):\n    spark.conf.set(\"spark.databricks.optimizer.dynamicPartitionPruning\", \"true\")\n\n    from delta.tables import DeltaTable\n    if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n        deltaTable = DeltaTable.forPath(spark, f\"{folder_path}/{table_name}\")\n        deltaTable.alias(\"tgt\").merge(input_df.alias(\"src\"), merge_condition).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    else:\n        input_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"delta\").saveAsTable(f\"{db_name}.{table_name}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b85eef3e-098e-49a0-bbe7-27b5a66fe099","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"common_functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
